# å¤šGPUæ˜¾å­˜ä¼˜åŒ–æŒ‡å—

## å½“å‰å¤šGPUé…ç½®çš„æ˜¾å­˜åˆ†æ

### DataParallel çš„æ˜¾å­˜ä½¿ç”¨

ä½¿ç”¨ DataParallel æ—¶ï¼Œæ¯ä¸ªGPUçš„æ˜¾å­˜å ç”¨åŒ…æ‹¬ï¼š

1. **æ¨¡å‹å‚æ•°**ï¼ˆæ¯ä¸ªGPUéƒ½æœ‰ä¸€ä»½å®Œæ•´å‰¯æœ¬ï¼‰ï¼š
   - GPT-2æ¨¡å‹ï¼š~500MBï¼ˆå¦‚æœä½¿ç”¨PEFTï¼Œä¼šå‡å°‘ï¼‰
   - Embeddingå±‚ï¼š~10-50MBï¼ˆå–å†³äºç”¨æˆ·/ç‰©å“æ•°é‡ï¼‰
   - MLPå±‚ï¼š~5-20MB
   - **æ€»è®¡ï¼š~500-600MB/GPU**

2. **ç¼“å­˜æ¨¡å¼ä¸‹çš„Metadata Embeddings**ï¼ˆæ¯ä¸ªGPUéƒ½æœ‰ä¸€ä»½ï¼‰ï¼š
   - User embeddings: `n_users Ã— embed_dim Ã— 4 bytes`
   - Item embeddings: `n_items Ã— embed_dim Ã— 4 bytes`
   - ML-1Mæ•°æ®é›†ï¼š~6000 users Ã— 64 Ã— 4 = ~1.5MB
   - ML-1Mæ•°æ®é›†ï¼š~4000 items Ã— 64 Ã— 4 = ~1MB
   - **æ€»è®¡ï¼š~2-3MB/GPU**

3. **è®­ç»ƒæ—¶çš„æ¿€æ´»å€¼ï¼ˆActivationsï¼‰**ï¼š
   - Batchæ•°æ®ï¼š`batch_size Ã— 3 Ã— 4 bytes`ï¼ˆusers, pos_items, neg_itemsï¼‰
   - Forward activationsï¼šå–å†³äºbatch sizeå’Œæ¨¡å‹å¤§å°
   - **æ¯ä¸ªGPUå¤„ç†ï¼š`batch_size / num_gpus` çš„æ•°æ®**

### æ˜¾å­˜å ç”¨ä¼°ç®—

å‡è®¾ä½¿ç”¨8å¼ GPUï¼Œbatch_size=2048ï¼š

- **æ¨¡å‹å‚æ•°**ï¼š~600MB/GPU
- **ç¼“å­˜embeddings**ï¼š~3MB/GPU
- **æ¯ä¸ªGPUçš„batch**ï¼š2048 / 8 = 256 samples
- **æ¿€æ´»å€¼**ï¼š~100-200MB/GPUï¼ˆå–å†³äºæ¨¡å‹å¤æ‚åº¦ï¼‰
- **æ¢¯åº¦**ï¼š~600MB/GPUï¼ˆä¸å‚æ•°ç›¸åŒï¼‰
- **ä¼˜åŒ–å™¨çŠ¶æ€**ï¼š~1200MB/GPUï¼ˆAdaméœ€è¦2å€å‚æ•°ç©ºé—´ï¼‰

**æ€»æ˜¾å­˜éœ€æ±‚**ï¼š~2.5-3GB/GPUï¼ˆç¼“å­˜æ¨¡å¼ï¼‰

## æ½œåœ¨é—®é¢˜

### âŒ å½“å‰é…ç½®çš„é—®é¢˜

1. **æ²¡æœ‰è‡ªåŠ¨è°ƒæ•´batch size**ï¼š
   - å¦‚æœbatch_sizeå¤ªå¤§ï¼Œæ¯ä¸ªGPUå¯èƒ½æ˜¾å­˜ä¸è¶³
   - éœ€è¦æ‰‹åŠ¨æ ¹æ®GPUæ•°é‡è°ƒæ•´

2. **ç¼“å­˜æ¨¡å¼æ˜¾å­˜å ç”¨å¤§**ï¼š
   - æ¯ä¸ªGPUéƒ½ä¼šç¼“å­˜metadata embeddings
   - å¦‚æœæ˜¾å­˜æœ‰é™ï¼Œå¯èƒ½å¯¼è‡´OOM

3. **æ²¡æœ‰æ˜¾å­˜ç›‘æ§**ï¼š
   - æ— æ³•æå‰å‘ç°æ˜¾å­˜é—®é¢˜
   - é”™è¯¯å‘ç”Ÿæ—¶å·²ç»å¤ªæ™š

4. **æ²¡æœ‰é”™è¯¯æ¢å¤æœºåˆ¶**ï¼š
   - OOMé”™è¯¯ä¼šå¯¼è‡´è®­ç»ƒä¸­æ–­
   - æ²¡æœ‰è‡ªåŠ¨é™çº§æ–¹æ¡ˆ

## âœ… ä¼˜åŒ–æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šè‡ªåŠ¨è°ƒæ•´Batch Sizeï¼ˆæ¨èï¼‰

```python
import torch

def get_optimal_batch_size(num_gpus, base_batch_size=2048):
    """
    æ ¹æ®GPUæ•°é‡è‡ªåŠ¨è°ƒæ•´batch size
    
    Args:
        num_gpus: GPUæ•°é‡
        base_batch_size: å•GPUçš„åŸºç¡€batch size
    
    Returns:
        è°ƒæ•´åçš„batch sizeï¼ˆæ¯ä¸ªGPUï¼‰
    """
    if num_gpus > 1:
        # å¤šGPUæ—¶ï¼Œæ¯ä¸ªGPUçš„batch size = æ€»batch size / GPUæ•°é‡
        per_gpu_batch = base_batch_size // num_gpus
        # ç¡®ä¿è‡³å°‘ä¸º1
        per_gpu_batch = max(1, per_gpu_batch)
        return per_gpu_batch
    return base_batch_size

# ä½¿ç”¨ç¤ºä¾‹
num_gpus = torch.cuda.device_count()
batch_size = get_optimal_batch_size(num_gpus, base_batch_size=2048)
print(f"ä½¿ç”¨ {num_gpus} å¼ GPUï¼Œæ¯ä¸ªGPUçš„batch size: {batch_size}")
```

### æ–¹æ¡ˆ2ï¼šä½¿ç”¨åŠ¨æ€æ¨¡å¼ï¼ˆå‡å°‘æ˜¾å­˜ï¼‰

```python
# é˜¶æ®µ1ï¼šä½¿ç”¨ç¼“å­˜æ¨¡å¼ï¼ˆå¿«é€Ÿä½†æ˜¾å­˜å ç”¨å¤§ï¼‰
model = GPT2RecommenderEnhanced(
    ...,
    use_cache=True,  # ç¼“å­˜æ¨¡å¼
    freeze_gpt2=True
)

# é˜¶æ®µ2ï¼šåˆ‡æ¢åˆ°åŠ¨æ€æ¨¡å¼ï¼ˆæ…¢ä½†æ˜¾å­˜å ç”¨å°ï¼‰
model.use_cache = False  # åŠ¨æ€æ¨¡å¼ï¼Œä¸ç¼“å­˜embeddings
```

### æ–¹æ¡ˆ3ï¼šæ¢¯åº¦ç´¯ç§¯ï¼ˆå¤„ç†å¤§batchï¼‰

å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¥æ¨¡æ‹Ÿæ›´å¤§çš„batch sizeï¼š

```python
# åœ¨trainerä¸­æ·»åŠ gradient_accumulation_stepså‚æ•°
trainer = Trainer(
    ...,
    gradient_accumulation_steps=4  # ç´¯ç§¯4ä¸ªbatchçš„æ¢¯åº¦
)
```

### æ–¹æ¡ˆ4ï¼šä½¿ç”¨æ›´å°‘çš„GPU

å¦‚æœæ˜¾å­˜ä»ç„¶ä¸è¶³ï¼Œå¯ä»¥é™åˆ¶ä½¿ç”¨çš„GPUæ•°é‡ï¼š

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'  # åªä½¿ç”¨4å¼ GPU
```

## æ¨èçš„æ˜¾å­˜ä¼˜åŒ–é…ç½®

### é…ç½®1ï¼šæ˜¾å­˜å……è¶³ï¼ˆ>8GB/GPUï¼‰

```python
batch_size = 2048  # æ€»batch size
use_cache = True   # ä½¿ç”¨ç¼“å­˜æ¨¡å¼
num_gpus = 8       # ä½¿ç”¨æ‰€æœ‰GPU
```

### é…ç½®2ï¼šæ˜¾å­˜ä¸­ç­‰ï¼ˆ4-8GB/GPUï¼‰

```python
batch_size = 1024  # å‡å°‘batch size
use_cache = True   # ä»å¯ä½¿ç”¨ç¼“å­˜
num_gpus = 4       # ä½¿ç”¨éƒ¨åˆ†GPU
```

### é…ç½®3ï¼šæ˜¾å­˜æœ‰é™ï¼ˆ<4GB/GPUï¼‰

```python
batch_size = 512   # å°batch size
use_cache = False  # ä½¿ç”¨åŠ¨æ€æ¨¡å¼
num_gpus = 2       # ä½¿ç”¨å°‘é‡GPU
```

## æ˜¾å­˜ç›‘æ§å’Œé”™è¯¯å¤„ç†

å»ºè®®æ·»åŠ æ˜¾å­˜ç›‘æ§ä»£ç ï¼š

```python
import torch

def check_gpu_memory():
    """æ£€æŸ¥GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            allocated = torch.cuda.memory_allocated(i) / 1024**3  # GB
            reserved = torch.cuda.memory_reserved(i) / 1024**3    # GB
            total = torch.cuda.get_device_properties(i).total_memory / 1024**3  # GB
            print(f"GPU {i}: {allocated:.2f}GB / {reserved:.2f}GB / {total:.2f}GB")

# åœ¨è®­ç»ƒå‰æ£€æŸ¥
check_gpu_memory()
```

## æ€»ç»“

### âœ… DataParallel çš„ä¼˜åŠ¿

1. **è‡ªåŠ¨åˆ†å‰²batch**ï¼šæ¯ä¸ªGPUå¤„ç† `batch_size / num_gpus` çš„æ•°æ®
2. **å‡å°‘å•GPUæ˜¾å­˜å‹åŠ›**ï¼šç›¸æ¯”å•GPUï¼Œæ¯ä¸ªGPUçš„æ˜¾å­˜éœ€æ±‚å‡å°‘
3. **ç®€å•æ˜“ç”¨**ï¼šæ— éœ€ä¿®æ”¹ä»£ç é€»è¾‘

### âš ï¸ éœ€è¦æ³¨æ„çš„é—®é¢˜

1. **æ¨¡å‹å¤åˆ¶**ï¼šæ¯ä¸ªGPUéƒ½æœ‰å®Œæ•´çš„æ¨¡å‹å‰¯æœ¬
2. **ç¼“å­˜embeddings**ï¼šå¦‚æœä½¿ç”¨ç¼“å­˜æ¨¡å¼ï¼Œæ¯ä¸ªGPUéƒ½ä¼šç¼“å­˜
3. **Batch sizeè°ƒæ•´**ï¼šéœ€è¦æ ¹æ®GPUæ•°é‡æ‰‹åŠ¨è°ƒæ•´

### ğŸ’¡ æœ€ä½³å®è·µ

1. **æ ¹æ®GPUæ˜¾å­˜è°ƒæ•´batch size**
2. **æ˜¾å­˜ä¸è¶³æ—¶ä½¿ç”¨åŠ¨æ€æ¨¡å¼**
3. **ç›‘æ§æ˜¾å­˜ä½¿ç”¨æƒ…å†µ**
4. **å‡†å¤‡é”™è¯¯æ¢å¤æœºåˆ¶**

